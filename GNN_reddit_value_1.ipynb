{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjpfMgdk9GDM",
        "outputId": "3bd5a9e1-ac03-4165-b06c-e0bc28f398db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fm1TpYx5D9En",
        "outputId": "26ea09fa-c1ab-4c09-aa9f-09729b1800f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp310-cp310-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp310-cp310-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=a0f0c2c4d8d1ef03e0d22900071073d9d3a72c48c85b4ff91678996fa214206b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.15)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=bb3fb03131835c23e3719158899482b2cf9935cc66e071d13993ab3d57f1870c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.3.1)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.2.0)\n",
            "Collecting networkx<3.0,>=2.5 (from node2vec)\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from node2vec) (1.22.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /usr/local/lib/python3.10/dist-packages (from node2vec) (4.65.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->node2vec) (6.3.0)\n",
            "Installing collected packages: networkx, node2vec\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "Successfully installed networkx-2.8.8 node2vec-0.4.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "networkx"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKz4CUhVEdYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d58f884-6730-44e4-8939-8dc4c61282e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZN2at4_ops6narrow4callERKNS_6TensorElll\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsMUXwuW_F4d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "f = open('reddit_edges.json')\n",
        "content = json.load(f)\n",
        "df = pd.read_csv('reddit_target.csv')\n",
        "\n",
        "G_list = list(content.values())\n",
        "Gs = [nx.Graph(i) for i in G_list[0:1000]]\n",
        "y = list(df.target)[0:1000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = list(Gs[0].nodes)\n",
        "# a = list(Gs[0].nodes)\n",
        "# a = np.array(a)\n",
        "# b = np.zeros((a.size, a.max()+1))\n",
        "# b[np.arange(a.size), a] = 1\n",
        "# b\n",
        "# c = torch.tensor(b, dtype=torch.float)\n",
        "# c\n",
        "b=[]\n",
        "for i in a:\n",
        "  b.append([1])\n",
        "b\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yasRfoFeaSB0",
        "outputId": "7b8709bf-ea69-401b-e05b-c461664cd9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1], [1], [1], [1], [1], [1], [1], [1], [1], [1], [1]]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt7SZKUEHPqS"
      },
      "outputs": [],
      "source": [
        "emb_list = []\n",
        "\n",
        "for i in Gs:\n",
        "  # a = list(i.nodes)\n",
        "  # a = np.array(a)\n",
        "  # b = np.zeros((a.size, a.max()+1))\n",
        "  # b[np.arange(a.size), a] = 1\n",
        "  # c = torch.tensor(b, dtype=torch.float)\n",
        "  c=[]\n",
        "  for a in list(i.nodes):\n",
        "    c.append([1])\n",
        "  c = torch.tensor(c, dtype=torch.float)\n",
        "  emb_list.append(c)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emb_list"
      ],
      "metadata": {
        "id": "7at3AP08ffwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWhc-gw5fGET"
      },
      "outputs": [],
      "source": [
        "y = torch.tensor([[i] for i in y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-iZ-UJ2KNc5"
      },
      "outputs": [],
      "source": [
        "edge_indices = []\n",
        "for i in G_list[0:1000]:\n",
        "  edge_indices.append(torch.tensor(i, dtype=torch.long).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX5w8eBrRX9t"
      },
      "outputs": [],
      "source": [
        "\n",
        "raw_data = list(zip(*(emb_list,edge_indices,y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0ZOluoASACO"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for i in raw_data:\n",
        "  data.append(Data(x=i[0], y=i[2], edge_index=i[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqQf7gTmTuHq"
      },
      "outputs": [],
      "source": [
        "trainset, testset= train_test_split(data, test_size=0.25, random_state=42)\n",
        "# trainset, valset= train_test_split(trainset, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain = []\n",
        "for i in trainset:\n",
        "  ytrain.append(i.y.numpy()[0])"
      ],
      "metadata": {
        "id": "4s6XSLHZk3uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest = []\n",
        "for i in testset:\n",
        "  ytest.append(i.y.numpy()[0])"
      ],
      "metadata": {
        "id": "23w5YAPtlTwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(ytest).value_counts().sort_values().plot(kind = 'barh')"
      ],
      "metadata": {
        "id": "_zfVmOqelii5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "b80f14ad-87a8-4d2c-fe52-ffd2c5b11f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMXm4WiNBDEU"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=0)\n",
        "# valid_loader = DataLoader(valset, batch_size=32, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(testset, batch_size=16, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[1].y"
      ],
      "metadata": {
        "id": "ZPdq_OjE9RTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf29679-aef4-48ef-eaf3-1c67e2393a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GraphConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(123)\n",
        "        self.conv1 = GraphConv(1, hidden_channels)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        # self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
        "        # self.bn3 = torch.nn.BatchNorm1d(hidden_channels)\n",
        "        self.hidden1 = Linear(hidden_channels, hidden_channels)\n",
        "        self.re1 = torch.nn.ReLU()\n",
        "        self.hidden2 = Linear(hidden_channels, hidden_channels)\n",
        "        self.re2 = torch.nn.ReLU()\n",
        "        self.lin = Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = x.relu()\n",
        "        # x = self.conv3(x, edge_index)\n",
        "        # x = self.bn3(x)\n",
        "        # x = x.relu()\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.re1(x)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.re2(x)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=256)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "mYGe_x0F8b2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7841d3-ffbe-40fb-c58d-813dfd54a0e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GraphConv(1, 256)\n",
            "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): GraphConv(256, 256)\n",
            "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (hidden1): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (re1): ReLU()\n",
            "  (hidden2): Linear(in_features=256, out_features=256, bias=True)\n",
            "  (re2): ReLU()\n",
            "  (lin): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=256)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  \n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 120):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "id": "RD_4_h-h9Xxn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "331dad33-6313-4259-8f2e-94786baa2d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.5053, Test Acc: 0.5120\n",
            "Epoch: 002, Train Acc: 0.5053, Test Acc: 0.5120\n",
            "Epoch: 003, Train Acc: 0.5053, Test Acc: 0.5120\n",
            "Epoch: 004, Train Acc: 0.5053, Test Acc: 0.5120\n",
            "Epoch: 005, Train Acc: 0.4987, Test Acc: 0.5000\n",
            "Epoch: 006, Train Acc: 0.5160, Test Acc: 0.5320\n",
            "Epoch: 007, Train Acc: 0.5773, Test Acc: 0.5960\n",
            "Epoch: 008, Train Acc: 0.5787, Test Acc: 0.6080\n",
            "Epoch: 009, Train Acc: 0.6000, Test Acc: 0.6160\n",
            "Epoch: 010, Train Acc: 0.6227, Test Acc: 0.6320\n",
            "Epoch: 011, Train Acc: 0.6307, Test Acc: 0.6360\n",
            "Epoch: 012, Train Acc: 0.6267, Test Acc: 0.6400\n",
            "Epoch: 013, Train Acc: 0.6293, Test Acc: 0.6200\n",
            "Epoch: 014, Train Acc: 0.6533, Test Acc: 0.6360\n",
            "Epoch: 015, Train Acc: 0.6520, Test Acc: 0.6440\n",
            "Epoch: 016, Train Acc: 0.6547, Test Acc: 0.6480\n",
            "Epoch: 017, Train Acc: 0.6493, Test Acc: 0.6360\n",
            "Epoch: 018, Train Acc: 0.6440, Test Acc: 0.6200\n",
            "Epoch: 019, Train Acc: 0.6533, Test Acc: 0.6360\n",
            "Epoch: 020, Train Acc: 0.6560, Test Acc: 0.6400\n",
            "Epoch: 021, Train Acc: 0.6533, Test Acc: 0.6240\n",
            "Epoch: 022, Train Acc: 0.6533, Test Acc: 0.6280\n",
            "Epoch: 023, Train Acc: 0.6480, Test Acc: 0.6240\n",
            "Epoch: 024, Train Acc: 0.6653, Test Acc: 0.6400\n",
            "Epoch: 025, Train Acc: 0.6507, Test Acc: 0.6240\n",
            "Epoch: 026, Train Acc: 0.6587, Test Acc: 0.6320\n",
            "Epoch: 027, Train Acc: 0.6600, Test Acc: 0.6400\n",
            "Epoch: 028, Train Acc: 0.6680, Test Acc: 0.6440\n",
            "Epoch: 029, Train Acc: 0.6480, Test Acc: 0.6280\n",
            "Epoch: 030, Train Acc: 0.6533, Test Acc: 0.6240\n",
            "Epoch: 031, Train Acc: 0.6600, Test Acc: 0.6280\n",
            "Epoch: 032, Train Acc: 0.6533, Test Acc: 0.6280\n",
            "Epoch: 033, Train Acc: 0.6507, Test Acc: 0.6320\n",
            "Epoch: 034, Train Acc: 0.6720, Test Acc: 0.6440\n",
            "Epoch: 035, Train Acc: 0.6600, Test Acc: 0.6320\n",
            "Epoch: 036, Train Acc: 0.6627, Test Acc: 0.6400\n",
            "Epoch: 037, Train Acc: 0.6733, Test Acc: 0.6440\n",
            "Epoch: 038, Train Acc: 0.6653, Test Acc: 0.6360\n",
            "Epoch: 039, Train Acc: 0.6720, Test Acc: 0.6400\n",
            "Epoch: 040, Train Acc: 0.6733, Test Acc: 0.6400\n",
            "Epoch: 041, Train Acc: 0.6680, Test Acc: 0.6360\n",
            "Epoch: 042, Train Acc: 0.6627, Test Acc: 0.6360\n",
            "Epoch: 043, Train Acc: 0.6680, Test Acc: 0.6360\n",
            "Epoch: 044, Train Acc: 0.6840, Test Acc: 0.6520\n",
            "Epoch: 045, Train Acc: 0.6773, Test Acc: 0.6400\n",
            "Epoch: 046, Train Acc: 0.6840, Test Acc: 0.6560\n",
            "Epoch: 047, Train Acc: 0.6733, Test Acc: 0.6560\n",
            "Epoch: 048, Train Acc: 0.6800, Test Acc: 0.6520\n",
            "Epoch: 049, Train Acc: 0.6880, Test Acc: 0.6640\n",
            "Epoch: 050, Train Acc: 0.6747, Test Acc: 0.6640\n",
            "Epoch: 051, Train Acc: 0.6707, Test Acc: 0.6520\n",
            "Epoch: 052, Train Acc: 0.6880, Test Acc: 0.6640\n",
            "Epoch: 053, Train Acc: 0.6787, Test Acc: 0.6480\n",
            "Epoch: 054, Train Acc: 0.6960, Test Acc: 0.6800\n",
            "Epoch: 055, Train Acc: 0.6760, Test Acc: 0.6560\n",
            "Epoch: 056, Train Acc: 0.6747, Test Acc: 0.6600\n",
            "Epoch: 057, Train Acc: 0.6787, Test Acc: 0.6560\n",
            "Epoch: 058, Train Acc: 0.6800, Test Acc: 0.6640\n",
            "Epoch: 059, Train Acc: 0.6813, Test Acc: 0.6520\n",
            "Epoch: 060, Train Acc: 0.6907, Test Acc: 0.6560\n",
            "Epoch: 061, Train Acc: 0.6760, Test Acc: 0.6360\n",
            "Epoch: 062, Train Acc: 0.6893, Test Acc: 0.6640\n",
            "Epoch: 063, Train Acc: 0.6800, Test Acc: 0.6440\n",
            "Epoch: 064, Train Acc: 0.6960, Test Acc: 0.6680\n",
            "Epoch: 065, Train Acc: 0.6960, Test Acc: 0.6640\n",
            "Epoch: 066, Train Acc: 0.6907, Test Acc: 0.6720\n",
            "Epoch: 067, Train Acc: 0.6960, Test Acc: 0.6640\n",
            "Epoch: 068, Train Acc: 0.6947, Test Acc: 0.6560\n",
            "Epoch: 069, Train Acc: 0.6760, Test Acc: 0.6440\n",
            "Epoch: 070, Train Acc: 0.6960, Test Acc: 0.6680\n",
            "Epoch: 071, Train Acc: 0.7000, Test Acc: 0.6680\n",
            "Epoch: 072, Train Acc: 0.6960, Test Acc: 0.6680\n",
            "Epoch: 073, Train Acc: 0.6920, Test Acc: 0.6640\n",
            "Epoch: 074, Train Acc: 0.6987, Test Acc: 0.6680\n",
            "Epoch: 075, Train Acc: 0.7013, Test Acc: 0.6720\n",
            "Epoch: 076, Train Acc: 0.6933, Test Acc: 0.6640\n",
            "Epoch: 077, Train Acc: 0.6933, Test Acc: 0.6520\n",
            "Epoch: 078, Train Acc: 0.7107, Test Acc: 0.7080\n",
            "Epoch: 079, Train Acc: 0.7013, Test Acc: 0.7000\n",
            "Epoch: 080, Train Acc: 0.6960, Test Acc: 0.6880\n",
            "Epoch: 081, Train Acc: 0.7107, Test Acc: 0.7000\n",
            "Epoch: 082, Train Acc: 0.7107, Test Acc: 0.7040\n",
            "Epoch: 083, Train Acc: 0.7067, Test Acc: 0.7000\n",
            "Epoch: 084, Train Acc: 0.7173, Test Acc: 0.7080\n",
            "Epoch: 085, Train Acc: 0.7120, Test Acc: 0.7000\n",
            "Epoch: 086, Train Acc: 0.7200, Test Acc: 0.6960\n",
            "Epoch: 087, Train Acc: 0.7187, Test Acc: 0.7000\n",
            "Epoch: 088, Train Acc: 0.7227, Test Acc: 0.7120\n",
            "Epoch: 089, Train Acc: 0.7160, Test Acc: 0.7040\n",
            "Epoch: 090, Train Acc: 0.7160, Test Acc: 0.7000\n",
            "Epoch: 091, Train Acc: 0.7107, Test Acc: 0.7040\n",
            "Epoch: 092, Train Acc: 0.7133, Test Acc: 0.7080\n",
            "Epoch: 093, Train Acc: 0.7107, Test Acc: 0.7040\n",
            "Epoch: 094, Train Acc: 0.7107, Test Acc: 0.6960\n",
            "Epoch: 095, Train Acc: 0.7200, Test Acc: 0.7000\n",
            "Epoch: 096, Train Acc: 0.7053, Test Acc: 0.6920\n",
            "Epoch: 097, Train Acc: 0.7080, Test Acc: 0.7000\n",
            "Epoch: 098, Train Acc: 0.7093, Test Acc: 0.7080\n",
            "Epoch: 099, Train Acc: 0.7173, Test Acc: 0.7160\n",
            "Epoch: 100, Train Acc: 0.7147, Test Acc: 0.7120\n",
            "Epoch: 101, Train Acc: 0.7053, Test Acc: 0.7000\n",
            "Epoch: 102, Train Acc: 0.7093, Test Acc: 0.7080\n",
            "Epoch: 103, Train Acc: 0.7133, Test Acc: 0.7240\n",
            "Epoch: 104, Train Acc: 0.7120, Test Acc: 0.7200\n",
            "Epoch: 105, Train Acc: 0.7120, Test Acc: 0.7160\n",
            "Epoch: 106, Train Acc: 0.7160, Test Acc: 0.7160\n",
            "Epoch: 107, Train Acc: 0.7160, Test Acc: 0.7120\n",
            "Epoch: 108, Train Acc: 0.7187, Test Acc: 0.7160\n",
            "Epoch: 109, Train Acc: 0.7107, Test Acc: 0.7160\n",
            "Epoch: 110, Train Acc: 0.7093, Test Acc: 0.7080\n",
            "Epoch: 111, Train Acc: 0.7133, Test Acc: 0.7200\n",
            "Epoch: 112, Train Acc: 0.7160, Test Acc: 0.7200\n",
            "Epoch: 113, Train Acc: 0.7227, Test Acc: 0.7160\n",
            "Epoch: 114, Train Acc: 0.7107, Test Acc: 0.7120\n",
            "Epoch: 115, Train Acc: 0.7120, Test Acc: 0.7200\n",
            "Epoch: 116, Train Acc: 0.7133, Test Acc: 0.7160\n",
            "Epoch: 117, Train Acc: 0.7173, Test Acc: 0.7000\n",
            "Epoch: 118, Train Acc: 0.7160, Test Acc: 0.7200\n",
            "Epoch: 119, Train Acc: 0.7133, Test Acc: 0.7120\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}